#!/usr/bin/env python3
"""
Ollama Diagnostic Script for Smart Legal Assistant
Reads Rag_config.json, tests connectivity, and validates model availability.
"""

import json
import sys
import requests
import os
import logging
from pathlib import Path
from typing import Dict, Any

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Disable proxies for local Ollama access to avoid corporate/local proxy interference
os.environ.setdefault('NO_PROXY', 'localhost,127.0.0.1')
os.environ.setdefault('no_proxy', 'localhost,127.0.0.1')

# Use a single session and ignore environment proxies entirely
SESSION = requests.Session()
SESSION.trust_env = False

def load_config(config_path: str = "phase_4_llm_rag/Rag_config.json") -> Dict[str, Any]:
    """Load RAG configuration from JSON file."""
    try:
        config_file = Path(config_path)
        if not config_file.exists():
            logger.error(f"âŒ ÙØ§ÛŒÙ„ ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÛŒØ§ÙØª Ù†Ø´Ø¯: {config_path}")
            return {}
        
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        logger.info(f"âœ… ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø² Ù…Ø³ÛŒØ± Ø²ÛŒØ± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯: {config_path}")
        return config
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª: {e}")
        return {}

def _detect_runtime_environment():
    """Detect the runtime environment (Windows, WSL, Docker) and suggest appropriate base_url."""
    import platform
    import os
    
    # Check for Docker environment
    if os.path.exists('/.dockerenv') or os.getenv('DOCKER_CONTAINER'):
        return "docker", "http://host.docker.internal:11434"
    
    # Check for WSL environment 
    if 'microsoft' in platform.uname().release.lower() or os.getenv('WSL_DISTRO_NAME'):
        return "wsl", "http://localhost:11434"
        
    # Check for Windows environment
    if platform.system() == "Windows":
        return "windows", "http://localhost:11434"
    
    # Default for Linux/macOS
    return "unix", "http://localhost:11434"


def _adaptive_base_url(configured_base_url: str) -> str:
    """Adapt base_url based on runtime environment if using default localhost."""
    # Only adapt if using default localhost URLs
    if configured_base_url not in ["http://localhost:11434", "http://127.0.0.1:11434"]:
        return configured_base_url
    
    env_type, suggested_url = _detect_runtime_environment()
    
    # For Docker, use host.docker.internal to reach host Ollama
    if env_type == "docker":
        logger.info(f"ğŸ³ Docker environment detected - adapting base_url to: {suggested_url}")
        return suggested_url
    
    # For other environments, keep localhost but ensure it's the right format
    return configured_base_url


def get_effective_config(config: Dict[str, Any]) -> Dict[str, str]:
    """Get effective Ollama configuration with environment precedence."""
    import os
    
    def _pick(env_key: str, cfg_dict: dict, cfg_key: str, default=None):
        """Helper for configuration precedence: env > config > default."""
        v = os.getenv(env_key)
        if v not in (None, ""):
            return v
        if cfg_dict and cfg_key in cfg_dict and cfg_dict[cfg_key] not in (None, ""):
            return cfg_dict[cfg_key]
        return default
    
    cfg = config.get("llm", {})
    
    # Get raw base_url and apply adaptive detection
    raw_base_url = _pick("OLLAMA_BASE_URL", cfg, "base_url", "http://localhost:11434")
    adaptive_base_url = _adaptive_base_url(raw_base_url)
    
    effective = {
        "provider": _pick("LLM_PROVIDER", cfg, "provider", "ollama"),
        "base_url": adaptive_base_url,
        "timeout": int(_pick("OLLAMA_TIMEOUT", cfg, "timeout_s", 60)),
        "model": _pick("OLLAMA_MODEL_NAME", cfg, "model", "unknown"),
        "backup_model": _pick("OLLAMA_BACKUP_MODEL", cfg, "backup_model", None)
    }
    
    return effective

def test_ollama_ping(base_url: str, timeout: int = 10) -> bool:
    """Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒâ€ŒÙ¾Ø°ÛŒØ±ÛŒ Ø³Ø±ÙˆØ± Ollama."""
    try:
        logger.info(f"ğŸ” Ø§Ø±Ø³Ø§Ù„ Ù¾ÛŒÙ†Ú¯ Ø¨Ù‡ Ø³Ø±ÙˆØ± Ollama Ø¯Ø± Ø¢Ø¯Ø±Ø³: {base_url}")
        
        response = SESSION.get(
            f"{base_url}/api/tags",
            timeout=timeout
        )
        
        if response.status_code == 200:
            logger.info(f"âœ… Ù¾ÛŒÙ†Ú¯ Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯: Ú©Ø¯ {response.status_code}")
            return True
        else:
            logger.error(f"âŒ Ù¾ÛŒÙ†Ú¯ Ù†Ø§Ù…ÙˆÙÙ‚: Ú©Ø¯ {response.status_code}")
            logger.error(f"   Ù¾Ø§Ø³Ø®: {response.text[:200]}")
            return False
            
    except requests.exceptions.ConnectTimeout:
        logger.error(f"âŒ Ù¾ÛŒÙ†Ú¯ Ù†Ø§Ù…ÙˆÙÙ‚: Ø§ØªÙ…Ø§Ù… Ø²Ù…Ø§Ù† Ø§ØªØµØ§Ù„ Ù¾Ø³ Ø§Ø² {timeout} Ø«Ø§Ù†ÛŒÙ‡")
        return False
    except requests.exceptions.ConnectionError as e:
        logger.error(f"âŒ Ù¾ÛŒÙ†Ú¯ Ù†Ø§Ù…ÙˆÙÙ‚: Ø®Ø·Ø§ÛŒ Ø§ØªØµØ§Ù„ - {e}")
        return False
    except requests.exceptions.Timeout:
        logger.error(f"âŒ Ù¾ÛŒÙ†Ú¯ Ù†Ø§Ù…ÙˆÙÙ‚: Ø§ØªÙ…Ø§Ù… Ø²Ù…Ø§Ù† Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ù¾Ø³ Ø§Ø² {timeout} Ø«Ø§Ù†ÛŒÙ‡")
        return False
    except Exception as e:
        logger.error(f"âŒ Ù¾ÛŒÙ†Ú¯ Ù†Ø§Ù…ÙˆÙÙ‚: {type(e).__name__}: {e}")
        return False

def get_available_models(base_url: str, timeout: int = 30) -> list:
    """Ø¯Ø±ÛŒØ§ÙØª ÙÙ‡Ø±Ø³Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø² Ollama."""
    try:
        logger.info(f"ğŸ“‹ Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª ÙÙ‡Ø±Ø³Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø§Ø² {base_url}/api/tags")
        
        response = SESSION.get(
            f"{base_url}/api/tags",
            timeout=timeout
        )
        
        if response.status_code == 200:
            data = response.json()
            models = []
            if "models" in data:
                for model_info in data["models"]:
                    if "name" in model_info:
                        models.append(model_info["name"].strip())
            
            logger.info(f"âœ… ØªØ¹Ø¯Ø§Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§: {len(models)}")
            return models
        else:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù…Ø¯Ù„â€ŒÙ‡Ø§: Ú©Ø¯ {response.status_code}")
            return []
            
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù…Ø¯Ù„â€ŒÙ‡Ø§: {e}")
        return []

def test_model_generate(base_url: str, model: str, timeout: int = 60) -> bool:
    """ØªØ³Øª ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ† ØªÙˆØ³Ø· Ù…Ø¯Ù„."""
    try:
        logger.info(f"ğŸ§ª ØªØ³Øª ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ† Ø¨Ø§ Ù…Ø¯Ù„: {model}")
        
        payload = {
            "model": model,
            "prompt": "Ø³Ù„Ø§Ù…",
            "stream": False,
            "options": {
                "temperature": 0.1,
                "num_predict": 1
            }
        }
        
        response = SESSION.post(
            f"{base_url}/api/generate",
            json=payload,
            timeout=timeout,
            headers={"Content-Type": "application/json"}
        )
        
        if response.status_code == 200:
            result = response.json()
            if "response" in result:
                generated = result["response"]
                logger.info(f"âœ… ØªÙˆÙ„ÛŒØ¯ Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯ (Ø·ÙˆÙ„ Ù¾Ø§Ø³Ø®: {len(generated)} Ú©Ø§Ø±Ø§Ú©ØªØ±)")
                logger.info(f"   Ù†Ù…ÙˆÙ†Ù‡ Ù¾Ø§Ø³Ø®: {generated[:100]}...")
                return True
            else:
                logger.error(f"âŒ ØªÙˆÙ„ÛŒØ¯ Ù†Ø§Ù…ÙˆÙÙ‚: Ú©Ù„ÛŒØ¯ 'response' Ø¯Ø± Ù†ØªÛŒØ¬Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
                return False
        else:
            logger.error(f"âŒ ØªÙˆÙ„ÛŒØ¯ Ù†Ø§Ù…ÙˆÙÙ‚: Ú©Ø¯ {response.status_code}")
            logger.error(f"   Ù¾Ø§Ø³Ø®: {response.text[:200]}")
            return False
            
    except requests.exceptions.Timeout:
        logger.error(f"âŒ ØªÙˆÙ„ÛŒØ¯ Ù†Ø§Ù…ÙˆÙÙ‚: Ø§ØªÙ…Ø§Ù… Ø²Ù…Ø§Ù† Ù¾Ø³ Ø§Ø² {timeout} Ø«Ø§Ù†ÛŒÙ‡")
        return False
    except Exception as e:
        logger.error(f"âŒ ØªÙˆÙ„ÛŒØ¯ Ù†Ø§Ù…ÙˆÙÙ‚: {e}")
        return False

def check_vector_store(config: Dict[str, Any]) -> bool:
    """Check if vector store files exist."""
    try:
        vector_config = config.get("vector_store", {})
        if not vector_config:
            logger.warning("âš ï¸ No vector_store config found")
            return False
        
        vs_type = vector_config.get("type", "unknown")
        logger.info(f"ğŸ“ Checking vector store: {vs_type}")
        
        if vs_type == "faiss":
            index_path = vector_config.get("index_path", "")
            embeddings_path = vector_config.get("embeddings_path", "")
            
            index_exists = Path(index_path).exists() if index_path else False
            embeddings_exist = Path(embeddings_path).exists() if embeddings_path else False
            
            logger.info(f"   Index file: {index_path} {'âœ…' if index_exists else 'âŒ'}")
            logger.info(f"   Embeddings: {embeddings_path} {'âœ…' if embeddings_exist else 'âŒ'}")
            
            return index_exists and embeddings_exist
        else:
            logger.warning(f"âš ï¸ Vector store type '{vs_type}' not supported in diagnostic")
            return False
            
    except Exception as e:
        logger.error(f"âŒ Vector store check failed: {e}")
        return False

def main():
    """Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ ØªØ´Ø®ÛŒØµÛŒ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ollama (Ø®Ø±ÙˆØ¬ÛŒ ÙØ§Ø±Ø³ÛŒ)."""
    print("ğŸ§ª Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¹ÛŒØ¨â€ŒÛŒØ§Ø¨ÛŒ Ollama Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªÛŒØ§Ø± Ø­Ù‚ÙˆÙ‚ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯")
    print("=" * 60)

    exit_code = 0

    # 1) Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª
    config = load_config()
    if not config:
        print("âŒ Ø®Ø·Ø§: Ø§Ù…Ú©Ø§Ù† Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
        return 1

    # 2) Ø§Ø¹Ù…Ø§Ù„ ØªÙ‚Ø¯Ù… ENV > config
    effective = get_effective_config(config)
    base_url = effective["base_url"]
    timeout = effective["timeout"]
    model = effective["model"]

    print("\nğŸ“Š ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…ÙˆØ«Ø±:")
    print(f"   Ø¢Ø¯Ø±Ø³ Ø³Ø±ÙˆØ± (base_url): {base_url}")
    print(f"   Ù†Ø§Ù… Ù…Ø¯Ù„ (model): {model}")

    # 3) ÙˆØ¶Ø¹ÛŒØª Ù¾ÛŒÙ†Ú¯
    print("\nğŸ” ÙˆØ¶Ø¹ÛŒØª Ù¾ÛŒÙ†Ú¯:")
    ping_ok = test_ollama_ping(base_url, min(timeout, 10))
    print("   Ù†ØªÛŒØ¬Ù‡ Ù¾ÛŒÙ†Ú¯:", "Ù…ÙˆÙÙ‚" if ping_ok else "Ù†Ø§Ù…ÙˆÙÙ‚")
    if not ping_ok:
        exit_code = 1

    # 4) ØªØ¹Ø¯Ø§Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø§Ø² /api/tags
    available_models = get_available_models(base_url, timeout)
    print("\nğŸ“‹ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§:")
    print(f"   ØªØ¹Ø¯Ø§Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯: {len(available_models)}")

    # 5) ØªØ³Øª ØªÙˆÙ„ÛŒØ¯ Û± ØªÙˆÚ©Ù† Ø¨Ø§ Ù…ØªÙ† 'Ø³Ù„Ø§Ù…'
    print("\nğŸ§ª ØªØ³Øª ØªÙˆÙ„ÛŒØ¯ ØªÚ©â€ŒØªÙˆÚ©Ù†:")
    if model in (None, "", "unknown"):
        print("   âŒ Ø®Ø·Ø§: Ù‡ÛŒÚ† Ù…Ø¯Ù„ÛŒ Ø¯Ø± ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø´Ø®Øµ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª (OLLAMA_MODEL_NAME ÛŒØ§ llm.model)")
        exit_code = 1
    else:
        can_generate = test_model_generate(base_url, model, timeout)
        print("   Ù†ØªÛŒØ¬Ù‡ ØªÙˆÙ„ÛŒØ¯:", "Ù…ÙˆÙÙ‚" if can_generate else "Ù†Ø§Ù…ÙˆÙÙ‚")
        if not can_generate:
            exit_code = 1

    # 6) Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ
    print("\nğŸ“‹ Ù†ØªÛŒØ¬Ù‡ Ù†Ù‡Ø§ÛŒÛŒ:")
    if exit_code == 0:
        print("âœ… Ù‡Ù…Ù‡ Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
    else:
        print("âŒ Ø¨Ø±Ø®ÛŒ Ø§Ø² Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯Ù†Ø¯. Ù„Ø·ÙØ§Ù‹ Ø®Ø·Ø§Ù‡Ø§ÛŒ ÙÙˆÙ‚ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.")

    return exit_code

if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n\nâš ï¸ Diagnostic interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ Diagnostic failed with error: {e}")
        sys.exit(1)
