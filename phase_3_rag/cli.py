#!/usr/bin/env python3
"""
CLI Ø¨Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ… RAG ÙØ§Ø±Ø³ÛŒ
Persian RAG System Command Line Interface

Ø§ÛŒÙ† Ø§Ø¨Ø²Ø§Ø± Ø®Ø· ÙØ±Ù…Ø§Ù† Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ø§Ù…Ù„ ÙØ±Ø¢ÛŒÙ†Ø¯ RAG Ø´Ø§Ù…Ù„ chunkingØŒ embedding generationØŒ
vector store building Ùˆ quality evaluation Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯.

This command line tool provides complete RAG pipeline management including
chunking, embedding generation, vector store building, and quality evaluation.
"""

import argparse
import os
import sys
import traceback
from pathlib import Path
from typing import Optional, Dict, Any

# Add project root to path for imports
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


def safe_print(message: str):
    """Print message safely handling Unicode encoding issues"""
    try:
        print(message)
    except UnicodeEncodeError:
        # Fallback to ASCII representation
        print(message.encode('ascii', errors='replace').decode('ascii'))

# Import shared utilities
try:
    from shared_utils.config_manager import ConfigManager
    from shared_utils.logger import get_logger
except ImportError as e:
    try:
        print(f"Error loading shared utilities: {e}")
    except UnicodeEncodeError:
        print("Error loading shared utilities")
    sys.exit(1)

# Import phase 3 modules
try:
    from phase_3_rag import chunker
    from phase_3_rag.embedding_generator import EmbeddingGenerator
    from phase_3_rag.vector_store_builder import VectorStoreBuilder
    from phase_3_rag.quality_eval import QualityEvaluator
except ImportError as e:
    try:
        print(f"Error loading phase 3 modules: {e}")
    except UnicodeEncodeError:
        print("Error loading phase 3 modules")
    sys.exit(1)


class PersianRAGCLI:
    """
    Ú©Ù„Ø§Ø³ CLI Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ… RAG ÙØ§Ø±Ø³ÛŒ
    Main CLI class for Persian RAG system
    """
    
    def __init__(self):
        """Initialize CLI with config and logger"""
        self.config_manager = ConfigManager()
        self.config = self.config_manager.get_config()
        
        # Setup logger
        self.logger = get_logger(
            name='phase3_cli',
            level='INFO'
        )
        
        self.logger.info("Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ CLI Ø³ÛŒØ³ØªÙ… RAG...")
        self.logger.info("Initializing RAG system CLI...")
    
    def create_parser(self) -> argparse.ArgumentParser:
        """
        Create argument parser with Persian help strings
        
        Returns:
            argparse.ArgumentParser: Configured parser
        """
        parser = argparse.ArgumentParser(
            prog='rag-cli',
            description='Ø§Ø¨Ø²Ø§Ø± Ø®Ø· ÙØ±Ù…Ø§Ù† Ø¨Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ… RAG ÙØ§Ø±Ø³ÛŒ',
            formatter_class=argparse.RawDescriptionHelpFormatter,
            epilog="""
Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡:
  python cli.py chunk --db data/db/legal_assistant.db --out data/processed_phase_3
  python cli.py embed --chunks data/processed_phase_3/chunks.json
  python cli.py build-index --backend faiss --out data/processed_phase_3/vector_db
  python cli.py eval --out .
  python cli.py all

Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ù‡Ø± Ø¯Ø³ØªÙˆØ±:
  python cli.py <command> --help
            """
        )
        
        subparsers = parser.add_subparsers(
            dest='command',
            title='Ø¯Ø³ØªÙˆØ±Ø§Øª Ù…ÙˆØ¬ÙˆØ¯',
            description='Ø¯Ø³ØªÙˆØ±Ø§Øª Ù‚Ø§Ø¨Ù„ Ø§Ø¬Ø±Ø§:',
            help='Ø§Ù†ØªØ®Ø§Ø¨ Ø¯Ø³ØªÙˆØ± Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§'
        )
        
        # Chunk command
        chunk_parser = subparsers.add_parser(
            'chunk',
            help='ØªØ¨Ø¯ÛŒÙ„ Ø§Ø³Ù†Ø§Ø¯ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ù‡ Ù‚Ø·Ø¹Ø§Øª Ù…ØªÙ†ÛŒ',
            description='Ø§ÛŒÙ† Ø¯Ø³ØªÙˆØ± Ø§Ø³Ù†Ø§Ø¯ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø±Ø§ Ø¨Ù‡ Ù‚Ø·Ø¹Ø§Øª Ú©ÙˆÚ†Ú©â€ŒØªØ± ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†Ø¯'
        )
        chunk_parser.add_argument(
            '--db',
            type=str,
            default='data/db/legal_assistant.db',
            help='Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: data/db/legal_assistant.db)'
        )
        chunk_parser.add_argument(
            '--out',
            type=str,
            default='data/processed_phase_3',
            help='Ù…Ø³ÛŒØ± Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù‚Ø·Ø¹Ø§Øª (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: data/processed_phase_3)'
        )
        
        # Embed command
        embed_parser = subparsers.add_parser(
            'embed',
            help='ØªÙˆÙ„ÛŒØ¯ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø§Ø² Ù‚Ø·Ø¹Ø§Øª',
            description='Ø§ÛŒÙ† Ø¯Ø³ØªÙˆØ± Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù‚Ø·Ø¹Ù‡ Ù…ØªÙ†ÛŒ ÛŒÚ© Ø¨Ø±Ø¯Ø§Ø± Ø¹Ø¯Ø¯ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯'
        )
        embed_parser.add_argument(
            '--chunks',
            type=str,
            default='data/processed_phase_3/chunks.json',
            help='Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ù‚Ø·Ø¹Ø§Øª JSON (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: data/processed_phase_3/chunks.json)'
        )
        embed_parser.add_argument(
            '--out',
            type=str,
            default='data/processed_phase_3',
            help='Ù…Ø³ÛŒØ± Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: data/processed_phase_3)'
        )
        
        # Build-index command
        index_parser = subparsers.add_parser(
            'build-index',
            help='Ø§ÛŒØ¬Ø§Ø¯ ÙÙ‡Ø±Ø³Øª Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§',
            description='Ø§ÛŒÙ† Ø¯Ø³ØªÙˆØ± Ø§Ø² Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ ÙÙ‡Ø±Ø³Øª Ø¬Ø³ØªØ¬ÙˆÛŒ Ø³Ø±ÛŒØ¹ Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯'
        )
        index_parser.add_argument(
            '--backend',
            type=str,
            choices=['faiss', 'chroma'],
            default='faiss',
            help='Ù†ÙˆØ¹ Ù¾Ø´ØªÛŒØ¨Ø§Ù† ÙÙ‡Ø±Ø³Øªâ€ŒØ³Ø§Ø²ÛŒ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: faiss)'
        )
        index_parser.add_argument(
            '--out',
            type=str,
            default='data/processed_phase_3/vector_db',
            help='Ù…Ø³ÛŒØ± Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ÙÙ‡Ø±Ø³Øª (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: data/processed_phase_3/vector_db)'
        )
        
        # Eval command
        eval_parser = subparsers.add_parser(
            'eval',
            help='Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª Ø³ÛŒØ³ØªÙ… RAG',
            description='Ø§ÛŒÙ† Ø¯Ø³ØªÙˆØ± Ú©ÛŒÙÛŒØª Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ Ùˆ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¬Ø³ØªØ¬Ùˆ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯'
        )
        eval_parser.add_argument(
            '--out',
            type=str,
            default='.',
            help='Ù…Ø³ÛŒØ± Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: .)'
        )
        
        # All command
        all_parser = subparsers.add_parser(
            'all',
            help='Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ ØªÙ…Ø§Ù… Ù…Ø±Ø§Ø­Ù„ Ø¨Ù‡ ØªØ±ØªÛŒØ¨',
            description='Ø§ÛŒÙ† Ø¯Ø³ØªÙˆØ± ØªÙ…Ø§Ù… Ù…Ø±Ø§Ø­Ù„ chunk â†’ embed â†’ build-index â†’ eval Ø±Ø§ Ø¨Ù‡ ØªØ±ØªÛŒØ¨ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯'
        )
        all_parser.add_argument(
            '--backend',
            type=str,
            choices=['faiss', 'chroma'],
            default='faiss',
            help='Ù†ÙˆØ¹ Ù¾Ø´ØªÛŒØ¨Ø§Ù† ÙÙ‡Ø±Ø³Øªâ€ŒØ³Ø§Ø²ÛŒ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: faiss)'
        )
        
        return parser
    
    def run_chunk(self, db_path: str, output_dir: str) -> bool:
        """
        Run chunking process
        
        Args:
            db_path: Database file path
            output_dir: Output directory for chunks
            
        Returns:
            bool: Success status
        """
        try:
            self.logger.info(f"Ø´Ø±ÙˆØ¹ ÙØ±Ø¢ÛŒÙ†Ø¯ chunking Ø¨Ø§ Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {db_path}")
            self.logger.info(f"Starting chunking process with database: {db_path}")
            
            # Check if database exists
            if not Path(db_path).exists():
                raise FileNotFoundError(f"ÙØ§ÛŒÙ„ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ ÛŒØ§ÙØª Ù†Ø´Ø¯: {db_path}")
            
            # Create output directory
            Path(output_dir).mkdir(parents=True, exist_ok=True)
            
            # Set environment for chunker
            original_dir = os.getcwd()
            try:
                os.chdir(output_dir)  # Change to output directory for chunker
                
                # Load chunking configuration
                config = chunker.load_chunking_config()
                
                # Create chunks from database
                chunks = chunker.create_chunks_from_database(db_path=db_path, config=config)
                
                if not chunks:
                    raise ValueError("No chunks were created from the database")
                
                # Write chunks to JSON file
                chunker.write_chunks_json(chunks)
                
            finally:
                os.chdir(original_dir)
            
            self.logger.info("ÙØ±Ø¢ÛŒÙ†Ø¯ chunking Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ú©Ø§Ù…Ù„ Ø´Ø¯")
            self.logger.info("Chunking process completed successfully")
            safe_print("âœ… ØªÙ‚Ø³ÛŒÙ…â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø³Ù†Ø§Ø¯ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯")
            return True
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ÙØ±Ø¢ÛŒÙ†Ø¯ chunking: {str(e)}")
            self.logger.error(f"Error in chunking process: {str(e)}")
            safe_print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªÙ‚Ø³ÛŒÙ…â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø³Ù†Ø§Ø¯: {str(e)}")
            return False
    
    def run_embed(self, chunks_path: str, output_dir: str) -> bool:
        """
        Run embedding generation process
        
        Args:
            chunks_path: Path to chunks JSON file
            output_dir: Output directory for embeddings
            
        Returns:
            bool: Success status
        """
        try:
            self.logger.info(f"Ø´Ø±ÙˆØ¹ ÙØ±Ø¢ÛŒÙ†Ø¯ ØªÙˆÙ„ÛŒØ¯ embedding Ø¨Ø§ Ù‚Ø·Ø¹Ø§Øª: {chunks_path}")
            self.logger.info(f"Starting embedding generation with chunks: {chunks_path}")
            
            # Check if chunks file exists
            if not Path(chunks_path).exists():
                raise FileNotFoundError(f"ÙØ§ÛŒÙ„ Ù‚Ø·Ø¹Ø§Øª ÛŒØ§ÙØª Ù†Ø´Ø¯: {chunks_path}")
            
            # Create output directory
            Path(output_dir).mkdir(parents=True, exist_ok=True)
            
            # Update config with paths
            config = self.config.copy()
            
            # Initialize and run embedding generator
            generator = EmbeddingGenerator(config)
            generator.chunks_path = Path(chunks_path)
            generator.embeddings_path = Path(output_dir) / "embeddings.npy"
            generator.meta_path = Path(output_dir) / "embeddings_meta.json"
            generator.run()
            
            self.logger.info("ÙØ±Ø¢ÛŒÙ†Ø¯ ØªÙˆÙ„ÛŒØ¯ embedding Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ú©Ø§Ù…Ù„ Ø´Ø¯")
            self.logger.info("Embedding generation process completed successfully")
            safe_print("âœ… ØªÙˆÙ„ÛŒØ¯ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯")
            return True
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ÙØ±Ø¢ÛŒÙ†Ø¯ ØªÙˆÙ„ÛŒØ¯ embedding: {str(e)}")
            self.logger.error(f"Error in embedding generation process: {str(e)}")
            safe_print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§: {str(e)}")
            return False
    
    def run_build_index(self, backend: str, output_dir: str) -> bool:
        """
        Run vector store building process
        
        Args:
            backend: Vector store backend (faiss/chroma)
            output_dir: Output directory for vector store
            
        Returns:
            bool: Success status
        """
        try:
            self.logger.info(f"Ø´Ø±ÙˆØ¹ ÙØ±Ø¢ÛŒÙ†Ø¯ Ø§ÛŒØ¬Ø§Ø¯ ÙÙ‡Ø±Ø³Øª Ø¨Ø§ Ù¾Ø´ØªÛŒØ¨Ø§Ù†: {backend}")
            self.logger.info(f"Starting index building with backend: {backend}")
            
            # Check if embeddings exist
            embeddings_path = Path("data/processed_phase_3/embeddings.npy")
            if not embeddings_path.exists():
                raise FileNotFoundError(f"ÙØ§ÛŒÙ„ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ ÛŒØ§ÙØª Ù†Ø´Ø¯: {embeddings_path}")
            
            # Create output directory
            Path(output_dir).mkdir(parents=True, exist_ok=True)
            
            # Update config with backend
            config = self.config.copy()
            config['rag'] = config.get('rag', {})
            config['rag']['index_backend'] = backend
            
            # Initialize and run vector store builder
            builder = VectorStoreBuilder(config)
            builder.vector_db_dir = Path(output_dir)
            builder.run()
            
            self.logger.info("ÙØ±Ø¢ÛŒÙ†Ø¯ Ø§ÛŒØ¬Ø§Ø¯ ÙÙ‡Ø±Ø³Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ú©Ø§Ù…Ù„ Ø´Ø¯")
            self.logger.info("Index building process completed successfully")
            safe_print("âœ… Ø§ÛŒØ¬Ø§Ø¯ ÙÙ‡Ø±Ø³Øª Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯")
            return True
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ÙØ±Ø¢ÛŒÙ†Ø¯ Ø§ÛŒØ¬Ø§Ø¯ ÙÙ‡Ø±Ø³Øª: {str(e)}")
            self.logger.error(f"Error in index building process: {str(e)}")
            safe_print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ÛŒØ¬Ø§Ø¯ ÙÙ‡Ø±Ø³Øª Ø¬Ø³ØªØ¬Ùˆ: {str(e)}")
            return False
    
    def run_eval(self, output_dir: str) -> bool:
        """
        Run quality evaluation process
        
        Args:
            output_dir: Output directory for evaluation reports
            
        Returns:
            bool: Success status
        """
        try:
            self.logger.info(f"Ø´Ø±ÙˆØ¹ ÙØ±Ø¢ÛŒÙ†Ø¯ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª")
            self.logger.info(f"Starting quality evaluation process")
            
            # Check if required files exist
            required_files = [
                "data/processed_phase_3/embeddings.npy",
                "data/processed_phase_3/embeddings_meta.json",
                "data/processed_phase_3/chunks.json"
            ]
            
            for file_path in required_files:
                if not Path(file_path).exists():
                    raise FileNotFoundError(f"ÙØ§ÛŒÙ„ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² ÛŒØ§ÙØª Ù†Ø´Ø¯: {file_path}")
            
            # Create output directory
            Path(output_dir).mkdir(parents=True, exist_ok=True)
            
            # Save current directory and change to output directory
            original_dir = os.getcwd()
            try:
                os.chdir(output_dir)
                
                # Initialize and run quality evaluator
                evaluator = QualityEvaluator(self.config)
                evaluator.run()
                
            finally:
                os.chdir(original_dir)
            
            self.logger.info("ÙØ±Ø¢ÛŒÙ†Ø¯ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ú©Ø§Ù…Ù„ Ø´Ø¯")
            self.logger.info("Quality evaluation process completed successfully")
            safe_print("âœ… Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯")
            return True
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ÙØ±Ø¢ÛŒÙ†Ø¯ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª: {str(e)}")
            self.logger.error(f"Error in quality evaluation process: {str(e)}")
            safe_print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª: {str(e)}")
            return False
    
    def run_all(self, backend: str) -> bool:
        """
        Run complete RAG pipeline
        
        Args:
            backend: Vector store backend (faiss/chroma)
            
        Returns:
            bool: Success status
        """
        try:
            self.logger.info("Ø´Ø±ÙˆØ¹ Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ† RAG")
            self.logger.info("Starting complete RAG pipeline execution")
            
            safe_print("ğŸš€ Ø´Ø±ÙˆØ¹ Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ø³ÛŒØ³ØªÙ… RAG...")
            safe_print("=" * 50)
            
            # Step 1: Chunking
            safe_print("Ù…Ø±Ø­Ù„Ù‡ Û±: ØªÙ‚Ø³ÛŒÙ…â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø³Ù†Ø§Ø¯...")
            if not self.run_chunk(
                db_path='data/db/legal_assistant.db',
                output_dir='data/processed_phase_3'
            ):
                return False
            
            # Step 2: Embedding generation
            safe_print("\nÙ…Ø±Ø­Ù„Ù‡ Û²: ØªÙˆÙ„ÛŒØ¯ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§...")
            if not self.run_embed(
                chunks_path='data/processed_phase_3/chunks.json',
                output_dir='data/processed_phase_3'
            ):
                return False
            
            # Step 3: Index building
            safe_print(f"\nÙ…Ø±Ø­Ù„Ù‡ Û³: Ø§ÛŒØ¬Ø§Ø¯ ÙÙ‡Ø±Ø³Øª Ø¬Ø³ØªØ¬Ùˆ ({backend})...")
            if not self.run_build_index(
                backend=backend,
                output_dir='data/processed_phase_3/vector_db'
            ):
                return False
            
            # Step 4: Quality evaluation
            safe_print("\nÙ…Ø±Ø­Ù„Ù‡ Û´: Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª...")
            if not self.run_eval(output_dir='.'):
                return False
            
            safe_print("\n" + "=" * 50)
            safe_print("ğŸ‰ ØªÙ…Ø§Ù… Ù…Ø±Ø§Ø­Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ú©Ø§Ù…Ù„ Ø´Ø¯!")
            self.logger.info("ØªÙ…Ø§Ù… Ù…Ø±Ø§Ø­Ù„ Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ† RAG Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ú©Ø§Ù…Ù„ Ø´Ø¯")
            self.logger.info("Complete RAG pipeline execution finished successfully")
            return True
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ†: {str(e)}")
            self.logger.error(f"Error in complete pipeline execution: {str(e)}")
            safe_print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ø³ÛŒØ³ØªÙ…: {str(e)}")
            return False
    
    def run(self, args: argparse.Namespace) -> int:
        """
        Run CLI command based on arguments
        
        Args:
            args: Parsed command line arguments
            
        Returns:
            int: Exit code (0 for success, non-zero for failure)
        """
        try:
            if args.command == 'chunk':
                success = self.run_chunk(args.db, args.out)
            elif args.command == 'embed':
                success = self.run_embed(args.chunks, args.out)
            elif args.command == 'build-index':
                success = self.run_build_index(args.backend, args.out)
            elif args.command == 'eval':
                success = self.run_eval(args.out)
            elif args.command == 'all':
                success = self.run_all(args.backend)
            else:
                safe_print("âŒ Ø¯Ø³ØªÙˆØ± Ù†Ø§Ù…Ø¹ØªØ¨Ø±. Ø§Ø² --help Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.")
                safe_print("âŒ Invalid command. Use --help for usage information.")
                return 1
            
            return 0 if success else 1
            
        except KeyboardInterrupt:
            safe_print("\nâ¸ï¸  Ø¹Ù…Ù„ÛŒØ§Øª ØªÙˆØ³Ø· Ú©Ø§Ø±Ø¨Ø± Ù…ØªÙˆÙ‚Ù Ø´Ø¯")
            safe_print("\nâ¸ï¸  Operation interrupted by user")
            self.logger.info("Ø¹Ù…Ù„ÛŒØ§Øª ØªÙˆØ³Ø· Ú©Ø§Ø±Ø¨Ø± Ù…ØªÙˆÙ‚Ù Ø´Ø¯")
            return 130
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡: {str(e)}")
            self.logger.error(f"Unexpected error: {str(e)}")
            self.logger.error(f"Traceback: {traceback.format_exc()}")
            safe_print(f"âŒ Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡: {str(e)}")
            return 1


def main() -> int:
    """
    Main entry point for CLI
    
    Returns:
        int: Exit code
    """
    try:
        # Initialize CLI
        cli = PersianRAGCLI()
        
        # Create and parse arguments
        parser = cli.create_parser()
        args = parser.parse_args()
        
        # Show help if no command provided
        if not args.command:
            parser.print_help()
            return 0
        
        # Run command
        return cli.run(args)
        
    except Exception as e:
        safe_print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ CLI: {str(e)}")
        safe_print(f"âŒ Error initializing CLI: {str(e)}")
        return 1


if __name__ == "__main__":
    exit(main())